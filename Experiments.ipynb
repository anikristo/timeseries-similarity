{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import py_ts_data\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "# Util functions\n",
    "def min_max(data, feature_range=(0, 1)):\n",
    "    \"\"\"\n",
    "    implements min-max scaler\n",
    "    \"\"\"\n",
    "    min_v = feature_range[0]\n",
    "    max_v = feature_range[1]\n",
    "    max_vals = data.max(axis=1)[:, None]\n",
    "    min_vals = data.min(axis=1)[:, None]\n",
    "    X_std = (data - min_vals) / (max_vals - min_vals)\n",
    "    return X_std * (max_v - min_v) + min_v\n",
    "\n",
    "def normalize(data):\n",
    "    \"\"\"\n",
    "    Z-normalize data with shape (x, y, z)\n",
    "    x = # of timeseries\n",
    "    y = len of each timeseries\n",
    "    z = vars in each timeseres\n",
    "    \n",
    "    s.t. each array in [., :, .] (i.e. each timeseries variable)\n",
    "    is zero-mean and unit stddev\n",
    "    \"\"\"\n",
    "    sz, l, d = data.shape\n",
    "    means = np.broadcast_to(np.mean(data, axis=1)[:, None], (sz, l, d))\n",
    "    stddev = np.broadcast_to(np.std(data, axis=1)[:, None], (sz, l, d)) \n",
    "    return (data - means)/stddev\n",
    "\n",
    "# Read in the data\n",
    "DATASET_NAME = \"TwoPatterns\"\n",
    "X_train, y_train, X_test, y_test, info = py_ts_data.load_data(DATASET_NAME, variables_as_channels=True)\n",
    "X_train = np.squeeze(X_train)\n",
    "X_test = np.squeeze(X_train)\n",
    "print(\"Selected dataset: {}\".format(DATASET_NAME))\n",
    "print(\"Dataset shape: Train: {}, Test: {}\".format(X_train.shape, X_test.shape))\n",
    "\n",
    "# Generate a preview of the dataset\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 3))\n",
    "axs[0].plot(X_train[0])\n",
    "axs[0].set_title(\"Training data\")\n",
    "X_train = min_max(X_train, feature_range=(-1, 1))\n",
    "axs[1].plot(X_test[0])\n",
    "axs[1].set_title(\"Testing data\")\n",
    "X_test = min_max(X_test, feature_range=(-1, 1))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from itertools import tee\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers,losses\n",
    "from tensorflow.signal import fft, ifft\n",
    "from tensorflow.math import conj\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Define the model\n",
    "class AutoEncoder(Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "\n",
    "        # Record params\n",
    "        input_shape = kwargs[\"input_shape\"]\n",
    "\n",
    "        # Define the encoders\n",
    "        self.first_encoder = tf.keras.Sequential([\n",
    "            layers.Input(shape=input_shape, dtype=tf.float64),\n",
    "            layers.Dense(128, activation='tanh', dtype=tf.float64),\n",
    "            ])\n",
    "\n",
    "        # NOTE: This has to be identical to the first encoder\n",
    "        self.second_encoder = tf.keras.Sequential([\n",
    "            layers.Input(shape=input_shape, dtype=tf.float64),\n",
    "            layers.Dense(128, activation='tanh', dtype=tf.float64),\n",
    "            ])\n",
    "\n",
    "        # Define the decoder\n",
    "        # NOTE: This has to be the inverted architecture of the encodersd\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.Dense(128, activation='tanh', dtype=tf.float64)\n",
    "        ])\n",
    "\n",
    "    def call(self, first_input, second_input, training=None):\n",
    "        first_code = self.first_encoder(first_input)\n",
    "        second_code = self.second_encoder(second_input)\n",
    "        first_output = self.decoder(first_code)\n",
    "        return first_code, second_code, first_output\n",
    "\n",
    "def SBD(X, Y):\n",
    "    # Takes two tensors X, Y, both with the same shape (batch_sz, ts_length)\n",
    "    # Returns a tensor Z with shape (batch_sz,), where Z[i] = sbd(X[i], Y[i])\n",
    "    assert len(X.shape) == 2\n",
    "    assert len(Y.shape) == 2\n",
    "    l = X.shape[1]\n",
    "    den = tf.reshape(tf.cast(tf.norm(X, axis=1) * tf.norm(Y, axis=1), dtype=tf.float64), (-1, 1))\n",
    "    den = tf.where(den != 0, den, float('inf'))\n",
    "    X = tf.cast(X, dtype=tf.complex64)\n",
    "    Y = tf.cast(Y, dtype=tf.complex64)\n",
    "    cc = ifft(fft(X) * conj(fft(Y)))\n",
    "    cc = tf.concat([cc[:, -(l-1):], cc[:, :l]], axis=1)\n",
    "    ncc = tf.cast(tf.math.real(cc), dtype=tf.float64)/den\n",
    "    Z = 1 - tf.reduce_max(ncc, axis=1)\n",
    "    return Z\n",
    "\n",
    "# Define the SBD loss\n",
    "class SBDLoss(losses.Loss):\n",
    "    \"\"\" Computes the Shape-based distances between two univariate timeseries \"\"\"\n",
    "\n",
    "    def __init__(self, name='sbd'):\n",
    "        super(SBDLoss, self).__init__(name=name)\n",
    "\n",
    "    def call(self, X, Y):\n",
    "        return SBD(X, Y)\n",
    "\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=0.00015)\n",
    "\n",
    "# Define the training function\n",
    "@tf.function\n",
    "def train_step(first_input, second_input, model, alpha=1, SBD_normalization_factor=0.1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Execute the model\n",
    "        first_code, second_code, first_output = model(first_input, second_input)\n",
    "        \n",
    "        # Calculate the Euclidean distance of the codes\n",
    "        codes_distance = SBD_normalization_factor * tf.norm(first_code - second_code, ord='euclidean', axis=1)\n",
    "\n",
    "        # Calculate the SBD of inputs\n",
    "        inputs_distance = SBD(first_input, second_input)\n",
    "\n",
    "        # Calculate the loss between the codes and inputs\n",
    "        similarity_loss = alpha * (losses.MeanSquaredError()(codes_distance, inputs_distance))\n",
    "\n",
    "        # Calculate the loss between the first input and the output\n",
    "        reconstruction_loss = tf.keras.losses.MeanSquaredError()(first_input, first_output)\n",
    "\n",
    "        # Calculate the total loss based on the weights\n",
    "        total_loss = similarity_loss + reconstruction_loss\n",
    "\n",
    "    gradients = tape.gradient([similarity_loss, reconstruction_loss], model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return similarity_loss, reconstruction_loss \n",
    "\n",
    "# Util: Pair creator\n",
    "def pairwise(iterable):\n",
    "    a, b = tee(iterable)\n",
    "    next(b, None)\n",
    "    return zip(a,b)\n",
    "\n",
    "# Training\n",
    "EPOCHS = 500\n",
    "BATCH = 50\n",
    "SHUFFLE_BUFFER = 100\n",
    "K = len(set(y_train))\n",
    "\n",
    "kwargs = {\n",
    "    \"input_shape\": (X_train.shape[1],),\n",
    "}\n",
    "\n",
    "ae = AutoEncoder(**kwargs)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER).batch(BATCH)\n",
    "\n",
    "similarity_loss_history = []\n",
    "reconstruction_loss_history = []\n",
    "\n",
    "print(\"Training...\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_similarity_loss = 0\n",
    "    total_reconstruction_loss = 0\n",
    "    for (first_input, _), (second_input, _) in pairwise(train_dataset):\n",
    "        similarity_loss, reconstruction_loss = train_step(first_input, second_input, ae)\n",
    "    total_similarity_loss += similarity_loss\n",
    "    total_reconstruction_loss += reconstruction_loss\n",
    "    similarity_loss_history.append(total_similarity_loss)\n",
    "    reconstruction_loss_history.append(total_reconstruction_loss)\n",
    "    \n",
    "plt.plot(similarity_loss_history, label=\"Similarity loss\")\n",
    "plt.plot(reconstruction_loss_history, label=\"Reconstruction loss\")\n",
    "plt.legend()\n",
    "\n",
    "print(\"DONE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Testing reconstruction\n",
    "code_test = ae.first_encoder(X_test)\n",
    "decoded_test = ae.decoder(code_test)\n",
    "\n",
    "plt.plot(X_test[0], label=\"input\")\n",
    "plt.plot(decoded_test[0], label=\"reconstruction\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "l2_losses = []\n",
    "for ground, predict in zip(X_test, decoded_test):\n",
    "    l2_losses.append(np.linalg.norm(ground - predict))\n",
    "print(\"Mean L2 reconstruction loss: {}\".format(np.array(l2_losses).mean()))\n",
    "\n",
    "sbd_losses = []\n",
    "for ground, predict in zip(X_test, decoded_test):\n",
    "    sbd_losses.append(SBD(tf.reshape(ground, shape=(1, -1)), tf.reshape(predict, shape=(1, -1))))\n",
    "print(\"Mean SBD reconstruction loss: {}\".format(np.array(sbd_losses).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Testing L2 similarity\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def nn_l2_dist(x, y):\n",
    "    \"\"\"\n",
    "    Sample distance metric, here, using only Euclidean distance\n",
    "    \"\"\"\n",
    "    return np.linalg.norm(x-y)\n",
    "\n",
    "# X_test = tf.squeeze(X_test)\n",
    "baseline_nn = NearestNeighbors(n_neighbors=10, metric=nn_l2_dist).fit(X_test)\n",
    "code_nn = NearestNeighbors(n_neighbors=10).fit(code_test)\n",
    "\n",
    "# For each item in the test data, find its 11 nearest neighbors in that dataset (the nn is itself)\n",
    "baseline_11nn = baseline_nn.kneighbors(X_test, 11, return_distance=False)\n",
    "code_11nn     = code_nn.kneighbors(code_test, 11, return_distance=False)\n",
    "\n",
    "# On average, how many common items are in the 10nn?\n",
    "result = []\n",
    "for b, c in zip(baseline_11nn, code_11nn):\n",
    "    # remove the first nn (itself)\n",
    "    b = set(b[1:])\n",
    "    c = set(c[1:])\n",
    "    result.append(len(b.intersection(c)))\n",
    "print(\"Mean L2 cluster size: {}\".format(np.array(result).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing SBD similarity\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def nn_sbd_dist(x, y):\n",
    "    return SBD(tf.reshape(x, shape=(1, -1)), tf.reshape(y, shape=(1, -1)))\n",
    "\n",
    "baseline_nn = NearestNeighbors(n_neighbors=10, metric=nn_sbd_dist).fit(X_test)\n",
    "code_nn = NearestNeighbors(n_neighbors=10).fit(code_test)\n",
    "\n",
    "# For each item in the test data, find its 11 nearest neighbors in that dataset (the nn is itself)\n",
    "baseline_11nn = baseline_nn.kneighbors(X_test, 11, return_distance=False)\n",
    "code_11nn     = code_nn.kneighbors(code_test, 11, return_distance=False)\n",
    "\n",
    "# On average, how many common items are in the 10nn?\n",
    "result = []\n",
    "for b, c in zip(baseline_11nn, code_11nn):\n",
    "    # remove the first nn (itself)\n",
    "    b = set(b[1:])\n",
    "    c = set(c[1:])\n",
    "    result.append(len(b.intersection(c)))\n",
    "print(\"Mean SBD cluster size: {}\".format(np.array(result).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "ae.first_encoder.save('sample_model/{}/encoder'.format(DATASET_NAME))\n",
    "ae.decoder.save('sample_model/{}/decoder'.format(DATASET_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}