{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import py_ts_data\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from itertools import tee\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers,losses\n",
    "from tensorflow.signal import fft, ifft\n",
    "from tensorflow.math import conj\n",
    "import tensorflow.keras.backend as K\n",
    "import dRNN\n",
    "\n",
    "# Util functions\n",
    "def min_max(data, feature_range=(0, 1)):\n",
    "    \"\"\"\n",
    "    implements min-max scaler\n",
    "    \"\"\"\n",
    "    min_v = feature_range[0]\n",
    "    max_v = feature_range[1]\n",
    "    max_vals = data.max(axis=1)[:, None]\n",
    "    min_vals = data.min(axis=1)[:, None]\n",
    "    X_std = (data - min_vals) / (max_vals - min_vals)\n",
    "    return X_std * (max_v - min_v) + min_v\n",
    "\n",
    "def normalize(data):\n",
    "    \"\"\"\n",
    "    Z-normalize data with shape (x, y, z)\n",
    "    x = # of timeseries\n",
    "    y = len of each timeseries\n",
    "    z = vars in each timeseres\n",
    "    \n",
    "    s.t. each array in [., :, .] (i.e. each timeseries variable)\n",
    "    is zero-mean and unit stddev\n",
    "    \"\"\"\n",
    "    sz, l, d = data.shape\n",
    "    means = np.broadcast_to(np.mean(data, axis=1)[:, None], (sz, l, d))\n",
    "    stddev = np.broadcast_to(np.std(data, axis=1)[:, None], (sz, l, d)) \n",
    "    return (data - means)/stddev\n",
    "\n",
    "# Util: Pair creator\n",
    "def pairwise(iterable):\n",
    "    a, b = tee(iterable)\n",
    "    next(b, None)\n",
    "    return zip(a,b)\n",
    "\n",
    "\n",
    "def SBD(X, Y):\n",
    "    # Takes two tensors X, Y, both with the same shape (batch_sz, ts_length)\n",
    "    # Returns a tensor Z with shape (batch_sz,), where Z[i] = sbd(X[i], Y[i])\n",
    "\n",
    "    assert len(X.shape) == 2\n",
    "    assert len(Y.shape) == 2\n",
    "\n",
    "    l = X.shape[1]\n",
    "    den = tf.reshape(tf.cast(tf.norm(X, axis=1) * tf.norm(Y, axis=1), dtype=tf.float64), (-1, 1))\n",
    "    den = tf.where(den != 0, den, float('inf'))\n",
    "    X = tf.cast(X, dtype=tf.complex64)\n",
    "    Y = tf.cast(Y, dtype=tf.complex64)\n",
    "    cc = ifft(fft(X) * conj(fft(Y)))\n",
    "    cc = tf.concat([cc[:, -(l-1):], cc[:, :l]], axis=1)\n",
    "    ncc = tf.cast(tf.math.real(cc), dtype=tf.float64)/den\n",
    "    Z = 1 - tf.reduce_max(ncc, axis=1)\n",
    "    return Z\n",
    "\n",
    "# Define the SBD loss\n",
    "class SBDLoss(losses.Loss):\n",
    "    \"\"\" Computes the Shape-based distances between two univariate timeseries \"\"\"\n",
    "\n",
    "    def __init__(self, name='sbd'):\n",
    "        super(SBDLoss, self).__init__(name=name)\n",
    "\n",
    "    def call(self, X, Y):\n",
    "        return SBD(X, Y)\n",
    "\n",
    "    \n",
    "\n",
    "# Read in the data\n",
    "DATASET_NAME = \"TwoPatterns\"\n",
    "X_train, y_train, X_test, y_test, info = py_ts_data.load_data(DATASET_NAME, variables_as_channels=True)\n",
    "# X_train = np.squeeze(X_train)\n",
    "# X_test = np.squeeze(X_train)\n",
    "print(\"Selected dataset: {}\".format(DATASET_NAME))\n",
    "print(\"Dataset shape: Train: {}, Test: {}\".format(X_train.shape, X_test.shape))\n",
    "\n",
    "# Generate a preview of the dataset\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 3))\n",
    "axs[0].plot(X_train[0])\n",
    "axs[0].set_title(\"Training data\")\n",
    "X_train = min_max(X_train, feature_range=(-1, 1))\n",
    "axs[1].plot(X_test[0])\n",
    "axs[1].set_title(\"Testing data\")\n",
    "X_test = min_max(X_test, feature_range=(-1, 1))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, opts):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_units = opts['encoder_hidden_units']\n",
    "        self.dilations = opts['dilations']\n",
    "        self.input_shape_ = opts['input_shape']\n",
    "        self.code_size = opts['code_size']\n",
    "        assert (len(self.hidden_units) == len(self.dilations))\n",
    "        self.cell_fw_list = [tf.keras.layers.GRUCell(units, dtype=tf.float64) for units in self.hidden_units]\n",
    "        self.drnn1 = dRNN.multi_dRNN_with_dilations(self.cell_fw_list, self.dilations)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.feed_forward_layer_1 = tf.keras.layers.Dense(self.code_size, activation=\"tanh\")\n",
    "        self.output_len = self.input_shape_[0]\n",
    "        self.output_channels = self.hidden_units[-1]\n",
    "        self.last_kernel_shape = (self.output_len, self.output_channels)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = tf.cast(inputs, tf.float64)\n",
    "        outputs_fw, states_fw = self.drnn1(inputs)\n",
    "        x = self.flatten(outputs_fw)\n",
    "        x = self.feed_forward_layer_1(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, opts):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.hidden_units = opts['encoder_hidden_units']\n",
    "        self.dilations = opts['dilations']\n",
    "        self.last_kernel_shape = opts['last_kernel_shape']\n",
    "\n",
    "        flat_len = self.last_kernel_shape[0] * self.last_kernel_shape[1]\n",
    "        self.expand = tf.keras.layers.Dense(flat_len)\n",
    "        self.reshape = tf.keras.layers.Reshape(self.last_kernel_shape)\n",
    "        self.cell_fw_list = [tf.keras.layers.GRUCell(units, dtype=tf.float64) for units in self.hidden_units]\n",
    "        self.drnn1 = dRNN.multi_dRNN_with_dilations(self.cell_fw_list, self.dilations)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = tf.cast(inputs, tf.float64)\n",
    "        x = self.expand(inputs)\n",
    "        x = self.reshape(x)\n",
    "        outputs_fw, states_fw = self.drnn1(x)\n",
    "        return outputs_fw\n",
    "\n",
    "\n",
    "# Define the model\n",
    "class AutoEncoder():\n",
    "    def __init__(self, **kwargs):\n",
    "        \n",
    "        input_shape = kwargs[\"input_shape\"]\n",
    "        code_size = kwargs[\"code_size\"]\n",
    "        filters = kwargs[\"filters\"]\n",
    "        kernel_sizes = kwargs[\"kernel_sizes\"]\n",
    "\n",
    "        config_dtcr = {}\n",
    "        config_dtcr['encoder_hidden_units'] = filters\n",
    "        config_dtcr['dilations'] = [1, 4, 16]\n",
    "        config_dtcr['input_shape'] = input_shape\n",
    "        config_dtcr['code_size'] = code_size\n",
    "\n",
    "        self.first_encoder = Encoder(opts=config_dtcr)\n",
    "        self.second_encoder = Encoder(opts=config_dtcr)\n",
    "\n",
    "        config_dtcr['encoder_hidden_units'] = list(config_dtcr['encoder_hidden_units'][:len(config_dtcr['encoder_hidden_units'])-1])\n",
    "        config_dtcr['encoder_hidden_units'].append(input_shape[1])\n",
    "        config_dtcr['last_kernel_shape'] = self.first_encoder.last_kernel_shape\n",
    "        self.decoder = Decoder(opts=config_dtcr)\n",
    "\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=0.001)\n",
    "\n",
    "# Define the training function\n",
    "@tf.function\n",
    "def train_step(first_input, second_input, model, alpha=1, SBD_normalization_factor=0.1):\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        # Execute the model\n",
    "        first_code = model.first_encoder(first_input)\n",
    "        second_code = model.second_encoder(second_input)\n",
    "        first_output = model.decoder(first_code)\n",
    "\n",
    "        # Calculate the Euclidean distance of the codes\n",
    "        codes_distance = SBD_normalization_factor * tf.norm(first_code - second_code, ord='euclidean', axis=1)\n",
    "\n",
    "        # Calculate the SBD of inputs\n",
    "        inputs_distance = SBD(tf.squeeze(first_input, axis=2), tf.squeeze(second_input, axis=2))\n",
    "\n",
    "        # Calculate the loss between the codes and inputs\n",
    "        similarity_loss = alpha * (losses.MeanSquaredError()(codes_distance, inputs_distance))\n",
    "\n",
    "        # Calculate the loss between the first input and the output\n",
    "        reconstruction_loss = tf.keras.losses.MeanSquaredError()(first_input, first_output)\n",
    "\n",
    "        # Calculate the total loss based on the weights\n",
    "        total_loss = similarity_loss + reconstruction_loss\n",
    "\n",
    "    trainables = model.first_encoder.trainable_variables + model.second_encoder.trainable_variables + model.decoder.trainable_variables \n",
    "    gradients = tape.gradient([similarity_loss, reconstruction_loss], trainables)\n",
    "    optimizer.apply_gradients(zip(gradients, trainables))\n",
    "    return similarity_loss, reconstruction_loss \n",
    "\n",
    "# Training\n",
    "EPOCHS = 500\n",
    "BATCH = 50\n",
    "SHUFFLE_BUFFER = 100\n",
    "K = len(set(y_train))\n",
    "\n",
    "kwargs = {\n",
    "    \"input_shape\": (X_train.shape[1],X_train.shape[2]),\n",
    "    \"filters\": [32, 64, 128],\n",
    "    \"kernel_sizes\": [5, 5, 5],\n",
    "    \"code_size\": 16,\n",
    "}\n",
    "\n",
    "ae = AutoEncoder(**kwargs)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER).batch(BATCH)\n",
    "\n",
    "similarity_loss_history = []\n",
    "reconstruction_loss_history = []\n",
    "\n",
    "print(\"Training...\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_similarity_loss = 0\n",
    "    total_reconstruction_loss = 0\n",
    "    for (first_input, _), (second_input, _) in pairwise(train_dataset):\n",
    "        similarity_loss, reconstruction_loss = train_step(first_input, second_input, ae)\n",
    "    total_similarity_loss += similarity_loss\n",
    "    total_reconstruction_loss += reconstruction_loss\n",
    "    similarity_loss_history.append(total_similarity_loss)\n",
    "    reconstruction_loss_history.append(total_reconstruction_loss)\n",
    "    \n",
    "plt.plot(similarity_loss_history, label=\"Similarity loss\")\n",
    "plt.plot(reconstruction_loss_history, label=\"Reconstruction loss\")\n",
    "plt.legend()\n",
    "\n",
    "print(\"DONE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Testing reconstruction\n",
    "code_test = ae.first_encoder(X_test)\n",
    "decoded_test = ae.decoder(code_test)\n",
    "\n",
    "plt.plot(X_test[0], label=\"input\")\n",
    "plt.plot(decoded_test[0], label=\"reconstruction\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "l2_losses = []\n",
    "for ground, predict in zip(X_test, decoded_test):\n",
    "    l2_losses.append(np.linalg.norm(ground - predict))\n",
    "print(\"Mean L2 reconstruction loss: {}\".format(np.array(l2_losses).mean()))\n",
    "\n",
    "sbd_losses = []\n",
    "for ground, predict in zip(X_test, decoded_test):\n",
    "    sbd_losses.append(SBD(tf.reshape(ground, shape=(1, -1)), tf.reshape(predict, shape=(1, -1))))\n",
    "print(\"Mean SBD reconstruction loss: {}\".format(np.array(sbd_losses).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Testing L2 similarity\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def nn_l2_dist(x, y):\n",
    "    \"\"\"\n",
    "    Sample distance metric, here, using only Euclidean distance\n",
    "    \"\"\"\n",
    "    return np.linalg.norm(x-y)\n",
    "\n",
    "# X_test = tf.squeeze(X_test)\n",
    "X_test = np.squeeze(X_test, axis=2)\n",
    "baseline_nn = NearestNeighbors(n_neighbors=10, metric=nn_l2_dist).fit(X_test)\n",
    "code_nn = NearestNeighbors(n_neighbors=10).fit(code_test)\n",
    "\n",
    "# For each item in the test data, find its 11 nearest neighbors in that dataset (the nn is itself)\n",
    "baseline_11nn = baseline_nn.kneighbors(X_test, 11, return_distance=False)\n",
    "code_11nn     = code_nn.kneighbors(code_test, 11, return_distance=False)\n",
    "\n",
    "# On average, how many common items are in the 10nn?\n",
    "result = []\n",
    "for b, c in zip(baseline_11nn, code_11nn):\n",
    "    # remove the first nn (itself)\n",
    "    b = set(b[1:])\n",
    "    c = set(c[1:])\n",
    "    result.append(len(b.intersection(c)))\n",
    "print(\"Mean L2 cluster size: {}\".format(np.array(result).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing SBD similarity\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def nn_sbd_dist(x, y):\n",
    "    return SBD(tf.reshape(x, shape=(1, -1)), tf.reshape(y, shape=(1, -1)))\n",
    "\n",
    "baseline_nn = NearestNeighbors(n_neighbors=10, metric=nn_sbd_dist).fit(X_test)\n",
    "code_nn = NearestNeighbors(n_neighbors=10).fit(code_test)\n",
    "\n",
    "# For each item in the test data, find its 11 nearest neighbors in that dataset (the nn is itself)\n",
    "baseline_11nn = baseline_nn.kneighbors(X_test, 11, return_distance=False)\n",
    "code_11nn     = code_nn.kneighbors(code_test, 11, return_distance=False)\n",
    "\n",
    "# On average, how many common items are in the 10nn?\n",
    "result = []\n",
    "for b, c in zip(baseline_11nn, code_11nn):\n",
    "    # remove the first nn (itself)\n",
    "    b = set(b[1:])\n",
    "    c = set(c[1:])\n",
    "    result.append(len(b.intersection(c)))\n",
    "print(\"Mean SBD cluster size: {}\".format(np.array(result).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "ae.first_encoder.save('sample_model/{}_dRNN/encoder'.format(DATASET_NAME))\n",
    "ae.decoder.save('sample_model/{}_dRNN/decoder'.format(DATASET_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}