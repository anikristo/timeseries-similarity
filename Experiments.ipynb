{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import py_ts_data\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from auto_encoder import AutoEncoder, train_step\n",
    "\n",
    "# Util functions\n",
    "def min_max(data, feature_range=(0, 1)):\n",
    "    \"\"\"\n",
    "    implements min-max scaler\n",
    "    \"\"\"\n",
    "    min_v = feature_range[0]\n",
    "    max_v = feature_range[1]\n",
    "    max_vals = data.max(axis=1)[:, None, :]\n",
    "    min_vals = data.min(axis=1)[:, None, :]\n",
    "    X_std = (data - min_vals) / (max_vals - min_vals)\n",
    "    return X_std * (max_v - min_v) + min_v\n",
    "\n",
    "def normalize(data):\n",
    "    \"\"\"\n",
    "    Z-normalize data with shape (x, y, z)\n",
    "    x = # of timeseries\n",
    "    y = len of each timeseries\n",
    "    z = vars in each timeseres\n",
    "    \n",
    "    s.t. each array in [., :, .] (i.e. each timeseries variable)\n",
    "    is zero-mean and unit stddev\n",
    "    \"\"\"\n",
    "    sz, l, d = data.shape\n",
    "    means = np.broadcast_to(np.mean(data, axis=1)[:, None, :], (sz, l, d))\n",
    "    stddev = np.broadcast_to(np.std(data, axis=1)[:, None, :], (sz, l, d)) \n",
    "    return (data - means)/stddev\n",
    "\n",
    "\n",
    "# Read in the data\n",
    "DATASET_NAME = \"Libras\"\n",
    "X_train, y_train, X_test, y_test, info = py_ts_data.load_data(DATASET_NAME, variables_as_channels=True)\n",
    "print(\"Selected dataset: {}\".format(DATASET_NAME))\n",
    "print(\"Dataset shape: Train: {}, Test: {}\".format(X_train.shape, X_test.shape))\n",
    "\n",
    "# Generate a preview of the dataset\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 3))\n",
    "axs[0].plot(X_train[0])\n",
    "axs[0].set_title(\"Training data\")\n",
    "X_train = min_max(X_train, feature_range=(-1, 1))\n",
    "axs[1].plot(X_test[0])\n",
    "axs[1].set_title(\"Testing data\")\n",
    "X_test = min_max(X_test, feature_range=(-1, 1))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training\n",
    "EPOCHS = 500\n",
    "BATCH = 64\n",
    "SHUFFLE_BUFFER = 100\n",
    "K = len(set(y_train))\n",
    "\n",
    "kwargs = {\n",
    "    \"input_shape\": (X_train.shape[1], X_train.shape[2]),\n",
    "    \"filters\": [32, 64, 128],\n",
    "    \"kernel_sizes\": [5, 5, 5],\n",
    "    \"code_size\": 16,\n",
    "}\n",
    "\n",
    "ae = AutoEncoder(**kwargs)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER).batch(BATCH)\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    for i, (input, _) in enumerate(train_dataset):\n",
    "        loss = train_step(input, ae)\n",
    "    total_loss += loss\n",
    "    loss_history.append(total_loss)\n",
    "    print(\"Epoch {}: {}\".format(epoch, total_loss), end=\"\\r\")\n",
    "    \n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Testing reconstruction\n",
    "code_test = ae.encode(X_test)\n",
    "decoded_test = ae.decode(code_test)\n",
    "\n",
    "plt.plot(X_test[0], label=\"input\")\n",
    "plt.plot(decoded_test[0], label=\"reconstruction\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "losses = []\n",
    "for ground, predict in zip(X_test, decoded_test):\n",
    "    losses.append(np.linalg.norm(ground - predict))\n",
    "print(\"Mean L2 reconstruction loss: {}\".format(np.array(losses).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Testing similarity\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def nn_dist(x, y):\n",
    "    \"\"\"\n",
    "    Sample distance metric, here, using only Euclidean distance\n",
    "    \"\"\"\n",
    "    x = x.reshape((45, 2))\n",
    "    y = y.reshape((45, 2))\n",
    "    return np.linalg.norm(x-y)\n",
    "\n",
    "nn_x_test = X_test.reshape((-1, 90))\n",
    "baseline_nn = NearestNeighbors(n_neighbors=10, metric=nn_dist).fit(nn_x_test)\n",
    "code_nn = NearestNeighbors(n_neighbors=10).fit(code_test)\n",
    "\n",
    "# For each item in the test data, find its 11 nearest neighbors in that dataset (the nn is itself)\n",
    "baseline_11nn = baseline_nn.kneighbors(nn_x_test, 11, return_distance=False)\n",
    "code_11nn     = code_nn.kneighbors(code_test, 11, return_distance=False)\n",
    "\n",
    "# On average, how many common items are in the 10nn?\n",
    "result = []\n",
    "for b, c in zip(baseline_11nn, code_11nn):\n",
    "    # remove the first nn (itself)\n",
    "    b = set(b[1:])\n",
    "    c = set(c[1:])\n",
    "    result.append(len(b.intersection(c)))\n",
    "print(\"Mean cluster size: {}\".format(np.array(result).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}