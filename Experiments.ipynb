{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import py_ts_data\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "# Util functions\n",
    "def min_max(data, feature_range=(0, 1)):\n",
    "    \"\"\"\n",
    "    implements min-max scaler\n",
    "    \"\"\"\n",
    "    min_v = feature_range[0]\n",
    "    max_v = feature_range[1]\n",
    "    max_vals = data.max(axis=1)[:, None]\n",
    "    min_vals = data.min(axis=1)[:, None]\n",
    "    X_std = (data - min_vals) / (max_vals - min_vals)\n",
    "    return X_std * (max_v - min_v) + min_v\n",
    "\n",
    "def normalize(data):\n",
    "    \"\"\"\n",
    "    Z-normalize data with shape (x, y, z)\n",
    "    x = # of timeseries\n",
    "    y = len of each timeseries\n",
    "    z = vars in each timeseres\n",
    "    \n",
    "    s.t. each array in [., :, .] (i.e. each timeseries variable)\n",
    "    is zero-mean and unit stddev\n",
    "    \"\"\"\n",
    "    sz, l, d = data.shape\n",
    "    means = np.broadcast_to(np.mean(data, axis=1)[:, None], (sz, l, d))\n",
    "    stddev = np.broadcast_to(np.std(data, axis=1)[:, None], (sz, l, d)) \n",
    "    return (data - means)/stddev\n",
    "\n",
    "# Read in the data\n",
    "DATASET_NAME = \"TwoPatterns\"\n",
    "X_train, y_train, X_test, y_test, info = py_ts_data.load_data(DATASET_NAME, variables_as_channels=True)\n",
    "X_train = np.squeeze(X_train)\n",
    "X_test = np.squeeze(X_train)\n",
    "print(\"Selected dataset: {}\".format(DATASET_NAME))\n",
    "print(\"Dataset shape: Train: {}, Test: {}\".format(X_train.shape, X_test.shape))\n",
    "\n",
    "# Generate a preview of the dataset\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 3))\n",
    "axs[0].plot(X_train[0])\n",
    "axs[0].set_title(\"Training data\")\n",
    "X_train = min_max(X_train, feature_range=(-1, 1))\n",
    "axs[1].plot(X_test[0])\n",
    "axs[1].set_title(\"Testing data\")\n",
    "X_test = min_max(X_test, feature_range=(-1, 1))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from itertools import tee\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers,losses\n",
    "from tensorflow.signal import fft, ifft\n",
    "from tensorflow.math import conj\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Define the model\n",
    "class AutoEncoder(Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "\n",
    "        # Record params\n",
    "        input_shape = kwargs[\"input_shape\"]\n",
    "\n",
    "        # Define the encoders\n",
    "        self.first_encoder = tf.keras.Sequential([\n",
    "            layers.Input(shape=input_shape, dtype=tf.float64),\n",
    "            # NOTE: We should have less than 128 nodes\n",
    "            layers.Dense(128, activation='tanh', dtype=tf.float64), \n",
    "            ])\n",
    "\n",
    "        # NOTE: This has to be identical to the first encoder\n",
    "        self.second_encoder = tf.keras.Sequential([\n",
    "            layers.Input(shape=input_shape, dtype=tf.float64),\n",
    "            # NOTE: We should have less than 128 nodes\n",
    "            layers.Dense(128, activation='tanh', dtype=tf.float64),\n",
    "            ])\n",
    "\n",
    "        # Define the decoder\n",
    "        # NOTE: This has to be the inverted architecture of the encodersd\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            # NOTE: The last Dense layer has to be 128\n",
    "            layers.Dense(128, activation='tanh', dtype=tf.float64)\n",
    "        ])\n",
    "\n",
    "    def call(self, first_input, second_input, training=None):\n",
    "        first_code = self.first_encoder(first_input)\n",
    "        second_code = self.second_encoder(second_input)\n",
    "        first_output = self.decoder(first_code)\n",
    "        return first_code, second_code, first_output\n",
    "\n",
    "def SBD(X, Y):\n",
    "    # Takes two tensors X, Y, both with the same shape (batch_sz, ts_length)\n",
    "    # Returns a tensor Z with shape (batch_sz,), where Z[i] = sbd(X[i], Y[i])\n",
    "    assert len(X.shape) == 2\n",
    "    assert len(Y.shape) == 2\n",
    "    l = X.shape[1]\n",
    "    den = tf.reshape(tf.cast(tf.norm(X, axis=1) * tf.norm(Y, axis=1), dtype=tf.float64), (-1, 1))\n",
    "    den = tf.where(den != 0, den, float('inf'))\n",
    "    X = tf.cast(X, dtype=tf.complex64)\n",
    "    Y = tf.cast(Y, dtype=tf.complex64)\n",
    "    cc = ifft(fft(X) * conj(fft(Y)))\n",
    "    cc = tf.concat([cc[:, -(l-1):], cc[:, :l]], axis=1)\n",
    "    ncc = tf.cast(tf.math.real(cc), dtype=tf.float64)/den\n",
    "    Z = 1 - tf.reduce_max(ncc, axis=1)\n",
    "    return Z\n",
    "\n",
    "# Define the SBD loss\n",
    "class SBDLoss(losses.Loss):\n",
    "    \"\"\" Computes the Shape-based distances between two univariate timeseries \"\"\"\n",
    "\n",
    "    def __init__(self, name='sbd'):\n",
    "        super(SBDLoss, self).__init__(name=name)\n",
    "\n",
    "    def call(self, X, Y):\n",
    "        return SBD(X, Y)\n",
    "\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=0.00015)\n",
    "\n",
    "# Define the training function\n",
    "@tf.function\n",
    "def train_step(first_input, second_input, model, alpha=1, SBD_normalization_factor=0.1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Execute the model\n",
    "        first_code, second_code, first_output = model(first_input, second_input)\n",
    "        \n",
    "        # Calculate the Euclidean distance of the codes\n",
    "        codes_distance = SBD_normalization_factor * tf.norm(first_code - second_code, ord='euclidean', axis=1)\n",
    "\n",
    "        # Calculate the SBD of inputs\n",
    "        inputs_distance = SBD(first_input, second_input)\n",
    "\n",
    "        # Calculate the loss between the codes and inputs\n",
    "        similarity_loss = alpha * (losses.MeanSquaredError()(codes_distance, inputs_distance))\n",
    "\n",
    "        # Calculate the loss between the first input and the output\n",
    "        reconstruction_loss = tf.keras.losses.MeanSquaredError()(first_input, first_output)\n",
    "\n",
    "        # Calculate the total loss based on the weights\n",
    "        total_loss = similarity_loss + reconstruction_loss\n",
    "\n",
    "    gradients = tape.gradient([similarity_loss, reconstruction_loss], model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return similarity_loss, reconstruction_loss \n",
    "\n",
    "# Util: Pair creator\n",
    "def pairwise(iterable):\n",
    "    a, b = tee(iterable)\n",
    "    next(b, None)\n",
    "    return zip(a,b)\n",
    "\n",
    "# Training\n",
    "EPOCHS = 500\n",
    "BATCH = 50\n",
    "SHUFFLE_BUFFER = 100\n",
    "K = len(set(y_train))\n",
    "\n",
    "kwargs = {\n",
    "    \"input_shape\": (X_train.shape[1],),\n",
    "}\n",
    "\n",
    "ae = AutoEncoder(**kwargs)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER).batch(BATCH)\n",
    "\n",
    "similarity_loss_history = []\n",
    "reconstruction_loss_history = []\n",
    "\n",
    "print(\"Training...\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_similarity_loss = 0\n",
    "    total_reconstruction_loss = 0\n",
    "    for (first_input, _), (second_input, _) in pairwise(train_dataset):\n",
    "        similarity_loss, reconstruction_loss = train_step(first_input, second_input, ae)\n",
    "    total_similarity_loss += similarity_loss\n",
    "    total_reconstruction_loss += reconstruction_loss\n",
    "    similarity_loss_history.append(total_similarity_loss)\n",
    "    reconstruction_loss_history.append(total_reconstruction_loss)\n",
    "    \n",
    "plt.plot(similarity_loss_history, label=\"Similarity loss\")\n",
    "plt.plot(reconstruction_loss_history, label=\"Reconstruction loss\")\n",
    "plt.legend()\n",
    "\n",
    "print(\"DONE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Testing reconstruction\n",
    "code_test = ae.first_encoder(X_test)\n",
    "decoded_test = ae.decoder(code_test)\n",
    "\n",
    "plt.plot(X_test[0], label=\"input\")\n",
    "plt.plot(decoded_test[0], label=\"reconstruction\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "l2_losses = []\n",
    "for ground, predict in zip(X_test, decoded_test):\n",
    "    l2_losses.append(np.linalg.norm(ground - predict))\n",
    "print(\"Mean L2 reconstruction loss: {}\".format(np.array(l2_losses).mean()))\n",
    "\n",
    "sbd_losses = []\n",
    "for ground, predict in zip(X_test, decoded_test):\n",
    "    sbd_losses.append(SBD(tf.reshape(ground, shape=(1, -1)), tf.reshape(predict, shape=(1, -1))))\n",
    "print(\"Mean SBD reconstruction loss: {}\".format(np.array(sbd_losses).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Testing L2 similarity\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def nn_l2_dist(x, y):\n",
    "    \"\"\"\n",
    "    Sample distance metric, here, using only Euclidean distance\n",
    "    \"\"\"\n",
    "    return np.linalg.norm(x-y)\n",
    "\n",
    "# X_test = tf.squeeze(X_test)\n",
    "baseline_nn = NearestNeighbors(n_neighbors=10, metric=nn_l2_dist).fit(X_test)\n",
    "code_nn = NearestNeighbors(n_neighbors=10).fit(code_test)\n",
    "\n",
    "# For each item in the test data, find its 11 nearest neighbors in that dataset (the nn is itself)\n",
    "baseline_11nn = baseline_nn.kneighbors(X_test, 11, return_distance=False)\n",
    "code_11nn     = code_nn.kneighbors(code_test, 11, return_distance=False)\n",
    "\n",
    "# On average, how many common items are in the 10nn?\n",
    "result = []\n",
    "for b, c in zip(baseline_11nn, code_11nn):\n",
    "    # remove the first nn (itself)\n",
    "    b = set(b[1:])\n",
    "    c = set(c[1:])\n",
    "    result.append(len(b.intersection(c)))\n",
    "print(\"Mean L2 cluster size: {}\".format(np.array(result).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-58f4ac5a1859>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# For each item in the test data, find its 11 nearest neighbors in that dataset (the nn is itself)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mbaseline_11nn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseline_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mcode_11nn\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mcode_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    660\u001b[0m                 \u001b[0mdelayed_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_tree_query_parallel_helper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m                 \u001b[0mparallel_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"prefer\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"threads\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m             chunked_results = Parallel(n_jobs, **parallel_kwargs)(\n\u001b[0m\u001b[1;32m    663\u001b[0m                 delayed_query(\n\u001b[1;32m    664\u001b[0m                     self._tree, X[s], n_neighbors, return_distance)\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1039\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1041\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36m_tree_query_parallel_helper\u001b[0;34m(tree, *args, **kwargs)\u001b[0m\n\u001b[1;32m    490\u001b[0m     \u001b[0munder\u001b[0m \u001b[0mPyPy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m     \"\"\"\n\u001b[0;32m--> 492\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32msklearn/neighbors/_binary_tree.pxi\u001b[0m in \u001b[0;36msklearn.neighbors._ball_tree.BinaryTree.query\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msklearn/neighbors/_binary_tree.pxi\u001b[0m in \u001b[0;36msklearn.neighbors._ball_tree.BinaryTree._query_single_depthfirst\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msklearn/neighbors/_binary_tree.pxi\u001b[0m in \u001b[0;36msklearn.neighbors._ball_tree.BinaryTree._query_single_depthfirst\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msklearn/neighbors/_binary_tree.pxi\u001b[0m in \u001b[0;36msklearn.neighbors._ball_tree.BinaryTree._query_single_depthfirst\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msklearn/neighbors/_binary_tree.pxi\u001b[0m in \u001b[0;36msklearn.neighbors._ball_tree.BinaryTree._query_single_depthfirst\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msklearn/neighbors/_binary_tree.pxi\u001b[0m in \u001b[0;36msklearn.neighbors._ball_tree.BinaryTree._query_single_depthfirst\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msklearn/neighbors/_binary_tree.pxi\u001b[0m in \u001b[0;36msklearn.neighbors._ball_tree.BinaryTree._query_single_depthfirst\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msklearn/neighbors/_binary_tree.pxi\u001b[0m in \u001b[0;36msklearn.neighbors._ball_tree.BinaryTree.rdist\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msklearn/neighbors/_dist_metrics.pyx\u001b[0m in \u001b[0;36msklearn.neighbors._dist_metrics.DistanceMetric.rdist\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msklearn/neighbors/_dist_metrics.pyx\u001b[0m in \u001b[0;36msklearn.neighbors._dist_metrics.PyFuncDistance.dist\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msklearn/neighbors/_dist_metrics.pyx\u001b[0m in \u001b[0;36msklearn.neighbors._dist_metrics.PyFuncDistance._dist\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-58f4ac5a1859>\u001b[0m in \u001b[0;36mnn_sbd_dist\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnn_sbd_dist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mSBD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mbaseline_nn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNearestNeighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn_sbd_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-74e4afb75bd5>\u001b[0m in \u001b[0;36mSBD\u001b[0;34m(X, Y)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mcc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mncc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mncc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mreduce_max\u001b[0;34m(input_tensor, axis, keepdims, name)\u001b[0m\n\u001b[1;32m   2743\u001b[0m     \u001b[0mThe\u001b[0m \u001b[0mreduced\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2744\u001b[0m   \"\"\"\n\u001b[0;32m-> 2745\u001b[0;31m   return reduce_max_with_dims(input_tensor, axis, keepdims, name,\n\u001b[0m\u001b[1;32m   2746\u001b[0m                               _ReductionDims(input_tensor, axis))\n\u001b[1;32m   2747\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mreduce_max_with_dims\u001b[0;34m(input_tensor, axis, keepdims, name, dims)\u001b[0m\n\u001b[1;32m   2755\u001b[0m   return _may_reduce_to_scalar(\n\u001b[1;32m   2756\u001b[0m       \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2757\u001b[0;31m       gen_math_ops._max(input_tensor, dims, keepdims, name=name))\n\u001b[0m\u001b[1;32m   2758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36m_max\u001b[0;34m(input, axis, keep_dims, name)\u001b[0m\n\u001b[1;32m   5608\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5609\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5610\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   5611\u001b[0m         _ctx, \"Max\", name, input, axis, \"keep_dims\", keep_dims)\n\u001b[1;32m   5612\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Testing SBD similarity\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def nn_sbd_dist(x, y):\n",
    "    return SBD(tf.reshape(x, shape=(1, -1)), tf.reshape(y, shape=(1, -1)))\n",
    "\n",
    "baseline_nn = NearestNeighbors(n_neighbors=10, metric=nn_sbd_dist).fit(X_test)\n",
    "code_nn = NearestNeighbors(n_neighbors=10).fit(code_test)\n",
    "\n",
    "# For each item in the test data, find its 11 nearest neighbors in that dataset (the nn is itself)\n",
    "baseline_11nn = baseline_nn.kneighbors(X_test, 11, return_distance=False)\n",
    "code_11nn     = code_nn.kneighbors(code_test, 11, return_distance=False)\n",
    "\n",
    "# On average, how many common items are in the 10nn?\n",
    "result = []\n",
    "for b, c in zip(baseline_11nn, code_11nn):\n",
    "    # remove the first nn (itself)\n",
    "    b = set(b[1:])\n",
    "    c = set(c[1:])\n",
    "    result.append(len(b.intersection(c)))\n",
    "print(\"Mean SBD cluster size: {}\".format(np.array(result).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "ae.first_encoder.save('sample_model/{}/encoder'.format(DATASET_NAME))\n",
    "ae.decoder.save('sample_model/{}/decoder'.format(DATASET_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}